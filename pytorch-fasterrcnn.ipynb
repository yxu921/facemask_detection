{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport torch\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport matplotlib.patches as patches\nimport os\nimport json\n\nimport cv2 \n\nimport pickle as pkl\nimport shutil\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-16T05:26:40.270746Z","iopub.execute_input":"2022-05-16T05:26:40.271096Z","iopub.status.idle":"2022-05-16T05:26:42.318982Z","shell.execute_reply.started":"2022-05-16T05:26:40.271065Z","shell.execute_reply":"2022-05-16T05:26:42.318062Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"is_training = False\nsave_tensors_to_plot = True\nuse_aug_data = False","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:26:42.321095Z","iopub.execute_input":"2022-05-16T05:26:42.321448Z","iopub.status.idle":"2022-05-16T05:26:42.325994Z","shell.execute_reply.started":"2022-05-16T05:26:42.321402Z","shell.execute_reply":"2022-05-16T05:26:42.325051Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:26:42.327374Z","iopub.execute_input":"2022-05-16T05:26:42.327793Z","iopub.status.idle":"2022-05-16T05:26:42.335599Z","shell.execute_reply.started":"2022-05-16T05:26:42.327744Z","shell.execute_reply":"2022-05-16T05:26:42.334840Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if not is_training:\n    !pip uninstall typing -y\n    !pip install object_detection_metrics\n    from podm import coco_decoder\n    from podm.metrics import get_pascal_voc_metrics, MetricPerClass, get_bounding_boxes, BoundingBox\n    from podm.box import Box, intersection_over_union","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:26:42.337119Z","iopub.execute_input":"2022-05-16T05:26:42.337589Z","iopub.status.idle":"2022-05-16T05:27:25.912669Z","shell.execute_reply.started":"2022-05-16T05:26:42.337543Z","shell.execute_reply":"2022-05-16T05:27:25.911781Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:25.916428Z","iopub.execute_input":"2022-05-16T05:27:25.916793Z","iopub.status.idle":"2022-05-16T05:27:25.981002Z","shell.execute_reply.started":"2022-05-16T05:27:25.916750Z","shell.execute_reply":"2022-05-16T05:27:25.979894Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Retrieve all the image and label files","metadata":{}},{"cell_type":"code","source":"test_label_path = \"../input/facemask-yangxu-dataset/testing_dataset/annotations\"\ntest_label_files = [os.path.join(test_label_path, file) for file in sorted(os.listdir(test_label_path))]\ntest_img_path = \"../input/facemask-yangxu-dataset/testing_dataset/images\"\ntest_img_files = [os.path.join(test_img_path, file) for file in sorted(os.listdir(test_img_path))]","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:25.984429Z","iopub.execute_input":"2022-05-16T05:27:25.985239Z","iopub.status.idle":"2022-05-16T05:27:26.060910Z","shell.execute_reply.started":"2022-05-16T05:27:25.985177Z","shell.execute_reply":"2022-05-16T05:27:26.060262Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if use_aug_data:\n    train_label_path = \"../input/facemask-yangxu-dataset/training_dataset_with_aug/annotations\"    \n    train_img_path = \"../input/facemask-yangxu-dataset/training_dataset_with_aug/images\"\nelse:\n    train_label_path = \"../input/facemask-yangxu-dataset/training_dataset/annotations\"    \n    train_img_path = \"../input/facemask-yangxu-dataset/training_dataset/images\"\n    \ntrain_label_files = [os.path.join(train_label_path, file) for file in sorted(os.listdir(train_label_path))]\ntrain_img_files = [os.path.join(train_img_path, file) for file in sorted(os.listdir(train_img_path))]    ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:26.062607Z","iopub.execute_input":"2022-05-16T05:27:26.063024Z","iopub.status.idle":"2022-05-16T05:27:26.462739Z","shell.execute_reply.started":"2022-05-16T05:27:26.062980Z","shell.execute_reply":"2022-05-16T05:27:26.461865Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ndef generate_box(obj):\n    \n    xmin = int(obj.find('xmin').text)\n    ymin = int(obj.find('ymin').text)\n    xmax = int(obj.find('xmax').text)\n    ymax = int(obj.find('ymax').text)\n    \n    return [xmin, ymin, xmax, ymax]\n\n\ndef generate_label(obj):\n    if obj.find('name').text == \"with_mask\":\n        return 1\n    elif obj.find('name').text == \"mask_weared_incorrect\":\n        return 2\n    elif obj.find('name').text == \"without_mask\":\n        return 3\n    return 0\n\ndef generate_target_xml(image_id, file):\n    with open(file) as f:\n        data = f.read()\n        soup = BeautifulSoup(data, 'xml')\n        objects = soup.find_all('object')\n\n        num_objs = len(objects)\n\n        # Bounding boxes for objects\n        # In coco format, bbox = [xmin, ymin, width, height]\n        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n        boxes = []\n        labels = []\n        for i in objects:\n            boxes.append(generate_box(i))\n            labels.append(generate_label(i))\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # Labels (In my case, I only one class: target class or background)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        # Tensorise img_id\n        img_id = torch.tensor([image_id])\n        # Annotation is in dictionary format\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = img_id\n        \n        return target\n    \ndef generate_target_json(image_id, file):\n    with open(file) as f:\n        data = json.load(f)\n    target = {}\n#     print(data)\n    target[\"boxes\"] = torch.as_tensor(data[\"boxes\"], dtype=torch.float32)\n    target[\"labels\"] = torch.as_tensor(data[\"labels\"], dtype=torch.int64)\n    target[\"image_id\"] = torch.tensor([image_id])\n    return target\n\n    \ndef generate_target(image_id, file): \n    if file.endswith(\".xml\"):\n        return generate_target_xml(image_id, file)\n    elif file.endswith(\".json\"):\n        return generate_target_json(image_id, file)\n    else:\n        raise ValueError(f\"Unrecognized file: {file}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:26.464418Z","iopub.execute_input":"2022-05-16T05:27:26.464987Z","iopub.status.idle":"2022-05-16T05:27:26.483768Z","shell.execute_reply.started":"2022-05-16T05:27:26.464932Z","shell.execute_reply":"2022-05-16T05:27:26.482659Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Build weight sampler","metadata":{}},{"cell_type":"code","source":"# # def generate_label(obj):\n# #     if obj.find('name').text == \"with_mask\":\n# #         return 1\n# #     elif obj.find('name').text == \"mask_weared_incorrect\":\n# #         return 2\n# #     elif obj.find('name').text == \"without_mask\":\n# #         return 3\n# #     return 0\n# if is_training:\n#     nclasses = 4\n#     count = [0] * nclasses                                                      \n#     training_labels = []\n#     def find_least_count_label(labels):\n#         if 2 in labels:\n#             return 2  \n#         if 3 in labels:\n#             return 3              \n#         if 1 in labels:\n#             return 1  \n#         return 0        \n\n#     for training_label_file in train_label_files:    \n#         target = generate_target(0, training_label_file)           \n#         for label in target[\"labels\"]:\n#             count[int(label)] += 1\n#         training_labels.append(target[\"labels\"])    \n#     weight_per_class = [0.] * 4      \n#     print(f\"count = {count}\")\n#     N = float(sum(count))                                                   \n#     for i in range(nclasses):  \n#         if count[i] == 0:\n#             weight_per_class[i] = 1e-10\n#         else:\n#             weight_per_class[i] = N/float(count[i])                                 \n#     weights = [0] * len(training_labels)                                              \n#     for idx, labels in enumerate(training_labels):           \n#         weights[idx] = weight_per_class[find_least_count_label(labels.detach().tolist())]                                  \n\n#     assert len(weights) == len(training_label_files)\n#     weights = torch.DoubleTensor(weights)    \n#     sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))                     \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:26.485181Z","iopub.execute_input":"2022-05-16T05:27:26.485640Z","iopub.status.idle":"2022-05-16T05:27:26.496022Z","shell.execute_reply.started":"2022-05-16T05:27:26.485602Z","shell.execute_reply":"2022-05-16T05:27:26.495211Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class MaskDataset(object):\n    def __init__(self, transforms, img_files, label_files):\n        self.transforms = transforms\n        assert len(img_files) == len(label_files)\n        self.imgs = img_files\n        self.labels = label_files\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = self.imgs[idx]\n        label_path = self.labels[idx]\n        assert os.path.exists(img_path) and os.path.exists(label_path)\n        img = Image.open(img_path).convert(\"RGB\")\n        #Generate Label\n        target = generate_target(idx, label_path)\n        \n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:26.497510Z","iopub.execute_input":"2022-05-16T05:27:26.497887Z","iopub.status.idle":"2022-05-16T05:27:26.510452Z","shell.execute_reply.started":"2022-05-16T05:27:26.497846Z","shell.execute_reply":"2022-05-16T05:27:26.509728Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_transform = transforms.Compose([\n        transforms.ToTensor(), \n    ])\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:26.513909Z","iopub.execute_input":"2022-05-16T05:27:26.514242Z","iopub.status.idle":"2022-05-16T05:27:26.520636Z","shell.execute_reply.started":"2022-05-16T05:27:26.514207Z","shell.execute_reply":"2022-05-16T05:27:26.519782Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataset = MaskDataset(data_transform, train_img_files, train_label_files)\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, collate_fn=collate_fn)\n    \ntest_dataset = MaskDataset(data_transform, test_img_files, test_label_files)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:26.522168Z","iopub.execute_input":"2022-05-16T05:27:26.522855Z","iopub.status.idle":"2022-05-16T05:27:26.529887Z","shell.execute_reply.started":"2022-05-16T05:27:26.522809Z","shell.execute_reply":"2022-05-16T05:27:26.529022Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def compute_dataset_statics(data_loader):\n    num_no_mask = 0\n    num_wear_mask = 0\n    num_wear_mask_wrong = 0\n    num_other = 0\n    for _, annotations in data_loader:        \n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        for annotation in annotations:\n            for label in annotation[\"labels\"].detach().tolist():\n                if label == 0:\n                    num_other += 1\n                elif label == 1:\n                    num_wear_mask += 1\n                elif label == 2:\n                    num_wear_mask_wrong += 1\n                elif label == 3:\n                    num_no_mask += 1\n                else:\n                    raise ValueError(f\"Unrecognized label: {label}\")\n    print(f\"DEBUG: num_no_mask_label = {num_no_mask}, num_wear_mask = {num_wear_mask}, num_wear_mask_wrong = {num_wear_mask_wrong}, num_other = {num_other}\")\n\ncompute_dataset_statics(train_data_loader)\ncompute_dataset_statics(test_data_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:26.531325Z","iopub.execute_input":"2022-05-16T05:27:26.531800Z","iopub.status.idle":"2022-05-16T05:27:58.589646Z","shell.execute_reply.started":"2022-05-16T05:27:26.531762Z","shell.execute_reply":"2022-05-16T05:27:58.588686Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:58.591865Z","iopub.execute_input":"2022-05-16T05:27:58.592412Z","iopub.status.idle":"2022-05-16T05:27:58.598483Z","shell.execute_reply.started":"2022-05-16T05:27:58.592369Z","shell.execute_reply":"2022-05-16T05:27:58.597563Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"num_classes = 4\nmodel = get_model_instance_segmentation(num_classes)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:27:58.600853Z","iopub.execute_input":"2022-05-16T05:27:58.601423Z","iopub.status.idle":"2022-05-16T05:28:01.678010Z","shell.execute_reply.started":"2022-05-16T05:27:58.601380Z","shell.execute_reply":"2022-05-16T05:28:01.675329Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"if not is_training:\n    torch.cuda.empty_cache()\n    model_path = \"../input/models/model.random_set.pt\"\n    model2 = get_model_instance_segmentation(num_classes)\n    if torch.cuda.is_available():\n        model2.load_state_dict(torch.load(model_path))\n    else:\n        model2.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n    model2.eval()\n    model2.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:28:01.679540Z","iopub.execute_input":"2022-05-16T05:28:01.679890Z","iopub.status.idle":"2022-05-16T05:28:04.245196Z","shell.execute_reply.started":"2022-05-16T05:28:01.679853Z","shell.execute_reply":"2022-05-16T05:28:04.244387Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh):\n    \"\"\"\n    Applies non max supression and eliminates low score bounding boxes.\n\n      Args:\n        orig_prediction: the model output. A dictionary containing element scores and boxes.\n        iou_thresh: Intersection over Union threshold. Every bbox prediction with an IoU greater than this value\n                      gets deleted in NMS.\n\n      Returns:\n        final_prediction: Resulting prediction\n    \"\"\"\n\n    # torchvision returns the indices of the bboxes to keep\n    keep_high_score = np.where(orig_prediction['scores'].cpu() >= 0.5)\n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep_high_score]\n    final_prediction['scores'] = final_prediction['scores'][keep_high_score]\n    final_prediction['labels'] = final_prediction['labels'][keep_high_score]        \n        \n    keep = torchvision.ops.nms(final_prediction['boxes'], final_prediction['scores'], iou_thresh)\n\n    # Keep indices from nms\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n\n    return final_prediction","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:28:04.246686Z","iopub.execute_input":"2022-05-16T05:28:04.247096Z","iopub.status.idle":"2022-05-16T05:28:04.256030Z","shell.execute_reply.started":"2022-05-16T05:28:04.247050Z","shell.execute_reply":"2022-05-16T05:28:04.255191Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"if not is_training:\n    IOU_threshold = 0.4\n    imgs_to_plot = []\n    annotations_to_plot = []\n    predictions_to_plot = []\n\n    torch.no_grad()\n    detection_boxes = []\n    ground_truth_boxes = []    \n    num_imgs_processed = 0\n    for imgs, annotations in test_data_loader:\n        num_imgs_processed += len(imgs)\n        testing_img_tensors = list(img.to(device) for img in imgs)\n        preds = model2(testing_img_tensors)\n        \n        assert len(preds) == len(annotations)\n        ######### How to find prediction with different label as annotation\n        for pred_idx, pred in enumerate(preds):\n            final_pred = apply_nms(pred, IOU_threshold)\n            labels = pred[\"labels\"].detach().tolist()\n#             print(f\"DEBUG: predicted labels = {labels}\")\n            scores = pred[\"scores\"].detach().tolist()  \n            for box_idx, box in enumerate(final_pred[\"boxes\"]):\n                xmin, ymin, xmax, ymax = box.tolist()\n                bb = BoundingBox.of_bbox(pred_idx, labels[box_idx], xmin, ymin, xmax, ymax, scores[box_idx])\n                detection_boxes.append(bb)\n        for annotation_idx, annotation in enumerate(annotations):\n            labels = annotation[\"labels\"].detach().tolist()\n            boxes = annotation[\"boxes\"].detach().tolist()    \n            for box_idx, box in enumerate(boxes):\n                xmin, ymin, xmax, ymax = box    \n                if labels[box_idx] == 2 and save_tensors_to_plot and len(imgs_to_plot) < 8:\n                    print(f\"Found label = 2, annotation_idx = {annotation_idx}\")\n                    imgs_to_plot.append(imgs[annotation_idx])\n                    annotations_to_plot.append(annotations[annotation_idx])\n                    final_pred = apply_nms(preds[annotation_idx], IOU_threshold)\n                    scores = final_pred[\"scores\"].detach().tolist()\n                    print(f\"DEBUG: scores = {scores}\")                        \n                    predictions_to_plot.append(final_pred)                \n                bb = BoundingBox.of_bbox(annotation_idx, labels[box_idx], xmin, ymin, xmax, ymax, 1.0)\n                ground_truth_boxes.append(bb)        \n#         print(f\"Num of imgs processed {num_imgs_processed}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:30:49.925312Z","iopub.execute_input":"2022-05-16T05:30:49.925806Z","iopub.status.idle":"2022-05-16T05:31:00.199213Z","shell.execute_reply.started":"2022-05-16T05:30:49.925760Z","shell.execute_reply":"2022-05-16T05:31:00.198299Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"if not is_training:\n    testing_results = get_pascal_voc_metrics(ground_truth_boxes, detection_boxes, .5)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:28:14.800570Z","iopub.execute_input":"2022-05-16T05:28:14.800901Z","iopub.status.idle":"2022-05-16T05:28:14.936629Z","shell.execute_reply.started":"2022-05-16T05:28:14.800865Z","shell.execute_reply":"2022-05-16T05:28:14.935947Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"if not is_training:\n    for label, metric in testing_results.items():\n        print(f\"**********{metric.label}***********\")\n        print('ap', metric.ap)\n#         print('precision', metric.precision)\n#         print('interpolated_recall', metric.interpolated_recall)\n#         print('interpolated_precision', metric.interpolated_precision)\n        print('tp', metric.tp)        \n        print('fp', metric.fp)\n        print('num_groundtruth', metric.num_groundtruth)\n        print('num_detection', metric.num_detection)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:28:14.938135Z","iopub.execute_input":"2022-05-16T05:28:14.938589Z","iopub.status.idle":"2022-05-16T05:28:14.949160Z","shell.execute_reply.started":"2022-05-16T05:28:14.938548Z","shell.execute_reply":"2022-05-16T05:28:14.947436Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"if is_training:\n    num_epochs = 5\n    model.to(device)\n\n    # parameters\n    torch.cuda.empty_cache()\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n    # Train a model with only the classifier loss\n    for epoch in range(num_epochs):\n        model.train()\n        i = 0    \n        epoch_loss = 0\n        for imgs, annotations in train_data_loader:\n            i += 1\n            imgs = list(img.to(device) for img in imgs)\n            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n            loss_dict = model(imgs, annotations)\n#             print({k: loss_dict[k].detach().tolist() for k in loss_dict.keys() })\n            losses = sum(loss for loss in loss_dict.values())        \n\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step() \n            epoch_loss += losses\n            if i % 50 == 0:\n                print('epoch = {}, i = {}'.format(epoch, i))\n        print(epoch_loss.detach().tolist())\n","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:28:14.950372Z","iopub.execute_input":"2022-05-16T05:28:14.950643Z","iopub.status.idle":"2022-05-16T05:28:14.965619Z","shell.execute_reply.started":"2022-05-16T05:28:14.950618Z","shell.execute_reply":"2022-05-16T05:28:14.964839Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"if is_training:\n    torch.save(model.state_dict(),'/kaggle/working/model.random_set.pt')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:28:14.967019Z","iopub.execute_input":"2022-05-16T05:28:14.967601Z","iopub.status.idle":"2022-05-16T05:28:14.974554Z","shell.execute_reply.started":"2022-05-16T05:28:14.967555Z","shell.execute_reply":"2022-05-16T05:28:14.973842Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Function to plot image","metadata":{}},{"cell_type":"code","source":"def plot_image(img_tensor, annotation):\n    \n    fig,ax = plt.subplots(1)\n    img = img_tensor.cpu().data\n\n    # Display the image\n    ax.imshow(img.permute(1, 2, 0))\n    \n    for idx, box in enumerate(annotation[\"boxes\"]):\n        xmin, ymin, xmax, ymax = box\n\n        # Create a Rectangle patch\n        label = annotation[\"labels\"][idx]\n        if label == 3:\n            color = \"r\"\n        elif label == 1:\n            color = \"g\"\n        elif label == 2:\n            color = \"y\"\n        else:\n            raise ValueError(f\"Annotation {label} incorrect!\")\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor=color,facecolor='none')\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:28:14.975977Z","iopub.execute_input":"2022-05-16T05:28:14.976531Z","iopub.status.idle":"2022-05-16T05:28:14.987085Z","shell.execute_reply.started":"2022-05-16T05:28:14.976494Z","shell.execute_reply":"2022-05-16T05:28:14.986033Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(len(imgs_to_plot))\nfor idx, img in enumerate(imgs_to_plot):\n    plot_image(imgs_to_plot[idx], annotations_to_plot[idx]) \ntorch.save(imgs_to_plot, \"/kaggle/working/imgs_to_plot.pt\")\ntorch.save(annotations_to_plot, \"/kaggle/working/annotations_to_plot.pt\")\ntorch.save(predictions_to_plot, \"/kaggle/working/predictions_to_plot.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:31:06.594588Z","iopub.execute_input":"2022-05-16T05:31:06.594928Z","iopub.status.idle":"2022-05-16T05:31:08.014463Z","shell.execute_reply.started":"2022-05-16T05:31:06.594896Z","shell.execute_reply":"2022-05-16T05:31:08.013733Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(\"Prediction\")\nfor idx, img in enumerate(imgs_to_plot):\n    plot_image(imgs_to_plot[idx], predictions_to_plot[idx])","metadata":{"execution":{"iopub.status.busy":"2022-05-16T05:31:17.489788Z","iopub.execute_input":"2022-05-16T05:31:17.490153Z","iopub.status.idle":"2022-05-16T05:31:19.069187Z","shell.execute_reply.started":"2022-05-16T05:31:17.490121Z","shell.execute_reply":"2022-05-16T05:31:19.068341Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}